{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建tensor\n",
    "torch.tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = torch.tensor(d)\n",
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2132550913984"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avoid copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(td1)=2132550913192\n",
      "id(td2)=2132550991944\n",
      "id(td3)=2132550991944\n"
     ]
    }
   ],
   "source": [
    "td1 = torch.tensor([1,2,3])\n",
    "print(f'id(td1)={id(td1)}')\n",
    "td2 = td1.detach()\n",
    "print(f'id(td2)={id(td2)}')\n",
    "td3 = torch.as_tensor(td2)\n",
    "print(f'id(td3)={id(td3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dtype\n",
    "tensor的默认类型是`torch.float`,有几种设置tensor类型的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3], dtype=torch.int32), 2132550992808)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td1 = torch.tensor([1,2,3], dtype=torch.int32)\n",
    "td1,id(td1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([True, True, True], dtype=torch.bool), 2132550994320)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td2 = td1.type(torch.bool)\n",
    "td2,id(td2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.], device='cuda:0'), 2132550994824)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td3 = td1.type(Tensor)\n",
    "td3,id(td3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用Tensor.to()进行类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.]), 2132550994104)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td4 = td1.to(torch.float)\n",
    "td4,id(td4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor转换为其他类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导出python number\n",
    "torch.Tensor.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.140000104904175"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = torch.tensor([3.14])\n",
    "pi = td.item()\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.14], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td1 = torch.tensor([1,2,3], dtype=torch.int32)\n",
    "npd = td.numpy()\n",
    "npd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in-place version\n",
    "in-place operation在pytorch中是指改变一个tensor的值的时候，不经过复制操作，而是直接在原来的内存上改变它的值。可以把它成为原地操作符。\n",
    "\n",
    "在pytorch中经常加后缀“_”来代表原地in-place operation，比如说.add_()。python里面的+=，*=也是in-place operation。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(td1)=2132551143928\n",
      "id(td)=2132551143928\n"
     ]
    }
   ],
   "source": [
    "td = torch.tensor([1,2,3])\n",
    "td1 = torch.tensor([4,5,6])\n",
    "print(f\"id(td)={id(td)}\")\n",
    "td = td.add_(td1)\n",
    "print(f\"id(td)={id(td)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(td)=2132551143928\n",
      "id(td)=2132551144360\n"
     ]
    }
   ],
   "source": [
    "td = torch.tensor([1,2,3])\n",
    "td1 = torch.tensor([4,5,6])\n",
    "print(f\"id(td)={id(td)}\")\n",
    "td = td.add(td1)\n",
    "print(f\"id(td)={id(td)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor shape相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensor shape\n",
    "```\n",
    "a = torch.randn(10,32,32,3)\n",
    "这个a可以解释为保存batch=10，size=32*32，channel为3的图像数据的变量。\n",
    "a有4个维度，我们将第一个维度大小为10，称为最高的维度，第四个维度大小为3，称为最低的维度，较低的维度的数据是“放在一起的/内存相连的”\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(10,32,32,3)\n",
    "a.dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view()\n",
    "view(a,b,c...)将根据第一个参数将内存划分a大块，然后每大块划分b中块，然后每中块划分b小块，依次类推...<br>\n",
    "如何给view()传入参数呢？要根据tensor存放数据方式对应的物理意义来决定。<br>\n",
    "例如tensor a存放了24个成员，包括了8个像素，每个像素用3通道数据表示，也就是我们将像素数量对应的维度定义为了高维度，通道数对应的维度定义为了低维度，那么通道数据是连在一起的，故该操作为：\n",
    "```\n",
    "a = a.view(8,3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\dp_pytorch11_GPU\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.range(1,9)\n",
    "print(a)\n",
    "a = a.view(3,3)\n",
    "print(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 3.],\n",
      "         [4., 5., 6.],\n",
      "         [7., 8., 9.]],\n",
      "\n",
      "        [[1., 2., 3.],\n",
      "         [4., 5., 6.],\n",
      "         [7., 8., 9.]]]) torch.Size([2, 3, 3])\n",
      "True\n",
      "tensor([[[1., 1.],\n",
      "         [2., 2.],\n",
      "         [3., 3.]],\n",
      "\n",
      "        [[4., 4.],\n",
      "         [5., 5.],\n",
      "         [6., 6.]],\n",
      "\n",
      "        [[7., 7.],\n",
      "         [8., 8.],\n",
      "         [9., 9.]]]) torch.Size([3, 3, 2])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "b = a.repeat(2,1,1)\n",
    "print(b,b.size())\n",
    "print(b.is_contiguous())\n",
    "b = b.permute(1,2,0)\n",
    "print(b,b.size())\n",
    "print(b.is_contiguous())\n",
    "b = b.contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### permute维度交换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(32,32,3)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1657, -1.1956,  1.2423], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Variable(a.type(Tensor))\n",
    "b[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b.permute(2,1,0)\n",
    "b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1657, -1.1956,  1.2423], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo2_view与permute的区别\n",
    "- view: view()操作的tensor必须是contiguous的，也就是按照内存地址的顺序根据输入的维度进行划分，但数据的存放顺序是完全没有改变的，仍然在连续的一块内存。\n",
    "- permute: permute()是更灵活的transpose()，用于维度的交换，该操作完成后，数据的不再按照原来地址存放。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\dp_pytorch11_GPU\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.range(1,6).view(2,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "b = a.view(3,2)\n",
    "print(b.is_contiguous())\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "c = a.permute(1,0)\n",
    "print(c.is_contiguous())\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "pre = torch.randn(10,3,13,13,8)\n",
    "x = pre[...,0]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]) torch.Size([13])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]]) torch.Size([13, 13])\n",
      "torch.Size([1, 1, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "g = 13\n",
    "grid_x = torch.arange(g)\n",
    "print(grid_x, grid_x.shape)\n",
    "grid_x = grid_x.repeat(g, 1)\n",
    "print(grid_x, grid_x.shape)\n",
    "grid_x = grid_x.view([1, 1, g, g]).type(torch.float)\n",
    "print(grid_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "res_x = x + grid_x\n",
    "print(res_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor索引\n",
    "对于多维的tensor来说，索引参数的size必须一样，如果不一样那么其size应该为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过比较运算符获取mask\n",
    "这种方式获取的mask与tensor具有相同size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 5, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,3,5,5,8)\n",
    "msk = x>0.5\n",
    "print(msk.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多维tensor提取\n",
    "x的size表示为(batch,anchor,width,width,channels)，如果希望根据特定条件，取出4个(msk的长度就是4)满足anchor条件、width条件、height条件的所有channel，则代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6896,  0.3553,  0.4264, -0.3482, -1.6622, -1.3962, -1.3688,  0.1051],\n",
      "        [-1.5447, -0.8086,  1.0541, -1.2533, -1.3357, -1.0364,  0.3644, -0.3640],\n",
      "        [ 0.2106, -0.0392, -2.4183, -1.2803,  1.0036, -0.0490, -1.3218, -0.3913],\n",
      "        [-0.0868, -1.2071,  1.4577, -0.3203,  0.0091,  1.3113, -0.6703, -0.2029]]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,3,5,5,8)\n",
    "msk1 = torch.tensor([0,1,0,1])\n",
    "msk2 = torch.tensor([1,0,1,2])\n",
    "msk3 = torch.tensor([0,4,1,2])\n",
    "msk4 = torch.tensor([1,4,3,0])\n",
    "y    = x[msk1,msk2,msk3,msk4]\n",
    "'''\n",
    "for i in range(len(msk1)):\n",
    "    res = x[msk1[i],msk2[i],msk3[i],msk4[i]]\n",
    "'''\n",
    "print(y,y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0, 0, 0],\n",
      "          [0, 0, 0, 1, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 1]],\n",
      "\n",
      "         [[0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [1, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0]]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "msk = torch.ByteTensor(4,3,5,5).fill_(0)\n",
    "msk[msk1,msk2,msk3,msk4] = 1\n",
    "print(msk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mask与tensor具有相同size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0105, -1.4868,  1.3388, -1.2086,  0.0069],\n",
      "        [ 0.4063, -0.7230, -0.9968, -0.4063, -0.6198],\n",
      "        [ 0.2121,  1.7841,  0.0084, -1.0090, -0.1287]])\n",
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,5)\n",
    "msk = torch.ByteTensor(3,5).fill_(1)\n",
    "print(x)\n",
    "print(msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0105, -1.4868,  1.3388, -1.2086,  0.0069,  0.4063, -0.7230, -0.9968,\n",
       "        -0.4063, -0.6198,  0.2121,  1.7841,  0.0084, -1.0090, -0.1287])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x[msk]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1408, -0.5380, -0.3321, -1.2923],\n",
       "        [-0.5694,  0.5125,  0.2685, -1.6011],\n",
       "        [ 0.8303, -1.1273,  1.2312,  1.4933],\n",
       "        [-0.0817,  0.3833,  1.1367, -0.5969],\n",
       "        [ 0.1217,  0.2332,  1.9962,  0.0047]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(5,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.return_types.max'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.1408, 0.5125, 1.4933, 1.1367, 1.9962])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.max(1)\n",
    "print(type(b))\n",
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.1408, 0.5125, 1.4933, 1.1367, 1.9962]),\n",
       "indices=tensor([0, 1, 3, 2, 2]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### empty():在GPU创建空白tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1657, -1.1956,  1.2423],\n",
       "        [ 0.0000,  0.0000,  0.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.empty(2,3,device=device)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor分割\n",
    "将一个tensor分割成多份，存在一个batch tensor中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mother = torch.randn(32,32,3,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "childs = torch.empty(4,8,8,3,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "childs[0] = mother[:8,:8,:]\n",
    "childs[1] = mother[8:16,8:16,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6308, -0.0839], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mother[0,0,-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
